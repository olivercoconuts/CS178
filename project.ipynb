{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import mltools as ml\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "from sklearn import *\n",
    "from sklearn import neural_network\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.genfromtxt(\"data/X_train.txt\",delimiter=None)\n",
    "Y = np.genfromtxt(\"data/Y_train.txt\",delimiter=None)\n",
    "Xte = np.genfromtxt('data/X_test.txt',delimiter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = X[:10000, :]\n",
    "# y = Y[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this for y_submit\n",
    "# Xtr, Xva, Ytr, Yva = model_selection.train_test_split(\n",
    "#     x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "Xtr = X\n",
    "Ytr = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('poly', PolynomialFeatures(degree=2, include_bias=True, interaction_only=False)), ('logistic', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build classifiers\n",
    "logistic_classifier = pipeline.Pipeline([\n",
    "    ('poly', preprocessing.PolynomialFeatures(degree=2, interaction_only=False)),\n",
    "    ('logistic', linear_model.LogisticRegression())])\n",
    "logistic_classifier.fit(Xtr, Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier = neighbors.KNeighborsClassifier(n_neighbors=10, weights=\"distance\",\n",
    "                                                n_jobs=-1)\n",
    "knn_classifier.fit(Xtr, Ytr)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=4, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_classifier = ensemble.RandomForestClassifier(\n",
    "                    n_estimators=1000, min_samples_leaf= 4, n_jobs=-1,oob_score=True)\n",
    "random_forest_classifier.fit(Xtr,Ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scale', StandardScaler(copy=True, with_mean=True, with_std=True)), ('nn', MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(280, 140), learning_rate='constant',\n",
       "       learning_r...e=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=True))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_classifier = pipeline.Pipeline([\n",
    "    (\"scale\", preprocessing.StandardScaler().fit(Xtr)),\n",
    "    (\"nn\", neural_network.MLPClassifier(\n",
    "        hidden_layer_sizes=(14*20, 14*10),\n",
    "        warm_start=True))])\n",
    "neural_network_classifier.fit(Xtr, Ytr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.5, loss='deviance', max_depth=10,\n",
       "              max_features=None, max_leaf_nodes=15,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=0, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_boosting_classifier = ensemble.GradientBoostingClassifier(n_estimators=100, learning_rate=0.5,\n",
    "     max_depth=10, max_leaf_nodes=15, random_state=0)\n",
    "gradient_boosting_classifier.fit(Xtr,Ytr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_roc = 0\n",
    "# index = 0\n",
    "# for i in range(2,30):\n",
    "#     classifier_list = [(logistic_classifier, 2),\n",
    "#                        (knn_classifier, 2),\n",
    "#                        (random_forest_classifier,27),\n",
    "#                        (gradient_boosting_classifier, 7),\n",
    "#                        (neural_network_classifier, 15)\n",
    "#                       ]\n",
    "#     y_validation_hat_list = []\n",
    "#     for classifier in classifier_list:\n",
    "#         y_validation_hat = classifier[0].predict_proba(Xva)[:,1]\n",
    "#         for weight in range(classifier[1]):\n",
    "#             y_validation_hat_list.append(y_validation_hat)\n",
    "#     y_validation_hat_average = np.mean(np.array(y_validation_hat_list), axis=0)\n",
    "#     roc = metrics.roc_auc_score(Yva, y_validation_hat_average)\n",
    "#     if roc > max_roc:\n",
    "#         max_roc = roc\n",
    "#         index = i\n",
    "# print(max_roc,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = [(logistic_classifier, 2),\n",
    "                       (knn_classifier, 2),\n",
    "                       (random_forest_classifier,27),\n",
    "                       (gradient_boosting_classifier, 7),\n",
    "                       (neural_network_classifier, 15)\n",
    "                      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validation_hat_list = []\n",
    "# for classifier in classifier_list:\n",
    "#     y_validation_hat = classifier[0].predict_proba(Xva)[:,1]\n",
    "#     for weight in range(classifier[1]):\n",
    "#         y_validation_hat_list.append(y_validation_hat)\n",
    "# y_validation_hat_average = np.mean(np.array(y_validation_hat_list), axis=0)\n",
    "# print(\"roc:\", metrics.roc_auc_score(Yva, y_validation_hat_average))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submision predict\n",
    "y_test_hat_list = []\n",
    "for classifier in classifier_list:\n",
    "    y_test_hat = classifier[0].predict_proba(Xte)[:,1]\n",
    "    for weight in range(classifier[1]):\n",
    "        y_test_hat_list.append(y_test_hat)\n",
    "y_test_hat_average = np.mean(np.array(y_test_hat_list), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"Y_submit.txt\",\n",
    "           np.vstack( (np.arange(len(y_test_hat_average)) , y_test_hat_average) ).T,\n",
    "           '%d, %.2f',header='ID,Prob1',comments='',delimiter=',');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
